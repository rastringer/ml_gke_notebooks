{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastringer/ml_gke_notebooks/blob/main/swin_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv76sbm3U2IT"
      },
      "source": [
        "\n",
        "### Training a larger Swin transformer model on T4 GPUs using GKE\n",
        "\n",
        "In this example, we will fine-tune any pretrained vision model for image classification on a custom dataset. We will add a randomly initialized classification head on top of a pre-trained encoder, then fine-tune the model on a labeled dataset.\n",
        "\n",
        "\n",
        "\n",
        "### Authenticate to GCP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23lc27zkU0SH"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adzxse4hVDFi"
      },
      "source": [
        "Let's make sure we have the Google Cloud CLI installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z0KNFeSUygk"
      },
      "outputs": [],
      "source": [
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg |  gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg\n",
        "\n",
        "!echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" |  tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
        "\n",
        "!apt-get update &&  apt-get install google-cloud-cli\n",
        "\n",
        "!apt-get update && apt-get install google-cloud-cli-gke-gcloud-auth-plugin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jekg86nZWuOk"
      },
      "source": [
        "Here's how we can install `kubectl` in the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEklYhgbWpOj"
      },
      "outputs": [],
      "source": [
        "! apt-get update && apt-get install -y apt-transport-https gnupg2\n",
        "! curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "! echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | tee -a /etc/apt/sources.list.d/kubernetes.list\n",
        "! apt-get update\n",
        "! apt-get install -y kubectl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set environment"
      ],
      "metadata": {
        "id": "HSU8KWuGFo0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"PROJECT_ID\"] = \"<your-project-id>\"\n",
        "os.environ[\"REGION\"] = \"<your-region>\"\n",
        "os.environ[\"ZONE\"] = \"<your-zone>\"\n",
        "os.environ[\"BUCKET_NAME\"] = \"<your-bucket>\"\n",
        "os.environ[\"VPC_NAME\"] = \"default\"\n",
        "os.environ[\"VPC_SUBNET\"] = \"default\""
      ],
      "metadata": {
        "id": "FxB-7YsSFk6L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud config set project <your-project-id>"
      ],
      "metadata": {
        "id": "KKPlxRLYM-5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY8atHd7VOod"
      },
      "outputs": [],
      "source": [
        "# Enable APIs\n",
        "! gcloud services enable iam.googleapis.com\n",
        "! gcloud services enable container.googleapis.com\n",
        "! gcloud services enable cloudbuild.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8tEBusj3uRH"
      },
      "source": [
        "### Create a GKE cluster\n",
        "\n",
        "There are many options when creating a GKE cluster, please see [here](https://cloud.google.com/sdk/gcloud/reference/container/clusters/create) for the full list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYhS2ghP0Q-d"
      },
      "outputs": [],
      "source": [
        "! gcloud container clusters create t4-demo --location ${REGION} \\\n",
        "  --workload-pool ${PROJECT_ID}.svc.id.goog \\\n",
        "  --enable-image-streaming \\\n",
        "  --enable-shielded-nodes \\\n",
        "  --shielded-secure-boot \\\n",
        "  --shielded-integrity-monitoring \\\n",
        "  --enable-ip-alias \\\n",
        "  --node-locations=${ZONE} \\\n",
        "  --network projects/$PROJECT_ID/global/networks/${VPC_NAME} \\\n",
        "  --subnetwork ${VPC_SUBNET} \\\n",
        "  --workload-pool=${PROJECT_ID}.svc.id.goog \\\n",
        "  --labels=\"ml-on-gke=t4-demo\" \\\n",
        "  --addons GcsFuseCsiDriver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b-sXCUk33zi9"
      },
      "source": [
        "Now that we have a cluster, we need to add a node pool with the T4 GPUs. Create a [node pool](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus) with T4 GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZvEEu793yfz"
      },
      "outputs": [],
      "source": [
        "! gcloud container node-pools create t4-pool \\\n",
        "  --accelerator type=nvidia-tesla-t4,count=1,gpu-driver-version=default \\\n",
        "  --machine-type=n1-standard-2 \\\n",
        "  --num-nodes=3 \\\n",
        "  --region=${REGION} \\\n",
        "  --cluster=t4-demo \\\n",
        "  --node-locations ${ZONE}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av_qSHrm3Nay"
      },
      "outputs": [],
      "source": [
        "! apt-get update && apt-get install -y apt-transport-https gnupg2\n",
        "! curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "! echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | tee -a /etc/apt/sources.list.d/kubernetes.list\n",
        "! apt-get update\n",
        "! apt-get install -y kubectl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Writefile\n",
        "\n",
        "We use `%%writefile`, a magic function for notebooks, to write our code to a file, `train.py` in this case."
      ],
      "metadata": {
        "id": "z9eVruo3Qc1X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "\n",
        "# Thanks to Hugging Face for their Swin example notebook here:\n",
        "# https://colab.sandbox.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb\n",
        "\n",
        "from google.cloud import storage\n",
        "\n",
        "# Set your Google Cloud Storage bucket name\n",
        "bucket_name = \"eurosat-data\"\n",
        "\n",
        "# Set the local directory where you want to copy the files\n",
        "local_directory = \"data\"\n",
        "\n",
        "# Initialize a client\n",
        "client = storage.Client()\n",
        "\n",
        "# Get the bucket\n",
        "bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "# List the objects in the bucket\n",
        "blobs = bucket.list_blobs()\n",
        "\n",
        "# Loop through the objects and download each one\n",
        "for blob in blobs:\n",
        "    destination_blob_path = local_directory + blob.name\n",
        "    blob.download_to_filename(destination_blob_path)\n",
        "    print(f\"Downloaded {blob.name} to {destination_blob_path}\")\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "# load a custom dataset from local/remote files or folders using the ImageFolder feature\n",
        "\n",
        "# option 1: local/remote files (supporting the following formats: tar, gzip, zip, xz, rar, zstd)\n",
        "# Change this to delete 'content' from file path when running on GKE vs colab\n",
        "dataset = load_dataset(\"imagefolder\", data_files=\"EuroSAT.zip\")\n",
        "\n",
        "from datasets import load_metric\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "labels = dataset[\"train\"].features[\"label\"].names\n",
        "label2id, id2label = dict(), dict()\n",
        "for i, label in enumerate(labels):\n",
        "    label2id[label] = i\n",
        "    id2label[i] = label\n",
        "\n",
        "id2label[2]\n",
        "\n",
        "from transformers import AutoImageProcessor\n",
        "\n",
        "model_checkpoint = \"microsoft/swin-tiny-patch4-window7-224\" # pre-trained model from which to fine-tune\n",
        "batch_size = 32 # batch size for training and evaluation\n",
        "\n",
        "image_processor  = AutoImageProcessor.from_pretrained(model_checkpoint)\n",
        "image_processor\n",
        "\n",
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "if \"height\" in image_processor.size:\n",
        "    size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
        "    crop_size = size\n",
        "    max_size = None\n",
        "elif \"shortest_edge\" in image_processor.size:\n",
        "    size = image_processor.size[\"shortest_edge\"]\n",
        "    crop_size = (size, size)\n",
        "    max_size = image_processor.size.get(\"longest_edge\")\n",
        "\n",
        "train_transforms = Compose(\n",
        "        [\n",
        "            RandomResizedCrop(crop_size),\n",
        "            RandomHorizontalFlip(),\n",
        "            ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "val_transforms = Compose(\n",
        "        [\n",
        "            Resize(size),\n",
        "            CenterCrop(crop_size),\n",
        "            ToTensor(),\n",
        "            normalize,\n",
        "        ]\n",
        "    )\n",
        "\n",
        "def preprocess_train(example_batch):\n",
        "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [\n",
        "        train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]\n",
        "    ]\n",
        "    return example_batch\n",
        "\n",
        "def preprocess_val(example_batch):\n",
        "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
        "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
        "    return example_batch\n",
        "\n",
        "# split up training into training + validation\n",
        "splits = dataset[\"train\"].train_test_split(test_size=0.1)\n",
        "train_ds = splits['train']\n",
        "val_ds = splits['test']\n",
        "\n",
        "train_ds.set_transform(preprocess_train)\n",
        "val_ds.set_transform(preprocess_val)\n",
        "\n",
        "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    ignore_mismatched_sizes = True, # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
        ")\n",
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-eurosat\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=True,\n",
        ")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# the compute_metrics function takes a Named Tuple as input:\n",
        "# predictions, which are the logits of the model as Numpy arrays,\n",
        "# and label_ids, which are the ground-truth labels as Numpy arrays.\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
        "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
        "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)\n",
        "\n",
        "import torch\n",
        "\n",
        "def collate_fn(examples):\n",
        "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
        "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        ")\n",
        "\n",
        "train_results = trainer.train()\n",
        "# rest is optional but nice to have\n",
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_metrics(\"train\", train_results.metrics)\n",
        "trainer.save_state()\n",
        "\n",
        "metrics = trainer.evaluate()\n",
        "# some nice to haves:\n",
        "trainer.log_metrics(\"eval\", metrics)\n",
        "trainer.save_metrics(\"eval\", metrics)"
      ],
      "metadata": {
        "id": "MzbUFoLXOGBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJE5DylbwBmw"
      },
      "source": [
        "### Dockerfile\n",
        "\n",
        "Kubernetes is built for running workloads that can be specified in Docker containers. The `Dockerfile` is a script used to build a container image, which is a lightweight and portable unit that can run applications and their dependencies in isolated environments.\n",
        "\n",
        "In our `Dockerfile`, we list the base image, the files we want to copy (eg `train.py`), install dependencies and set environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND_IUMZiBYpc"
      },
      "outputs": [],
      "source": [
        "%%writefile Dockerfile\n",
        "\n",
        "FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04\n",
        "\n",
        "RUN apt-get update && \\\n",
        "    apt-get -y --no-install-recommends install python3-dev gcc python3-pip git && \\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "RUN pip3 install --no-cache-dir torch torchvision transformers peft datasets accelerate google-cloud-storage google-cloud-logging\n",
        "\n",
        "COPY train.py /train.py\n",
        "\n",
        "ENV PYTHONUNBUFFERED 1\n",
        "\n",
        "CMD python3 /train.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID-fTeG_xB1V"
      },
      "source": [
        "### Respository\n",
        "\n",
        "We need a repository to host our Docker image. In this example, we will use GCP's [Artifact Registry](https://cloud.google.com/artifact-registry)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fycbJHyBY2L"
      },
      "outputs": [],
      "source": [
        "! gcloud artifacts repositories create pytorch-t4 \\\n",
        "    --project=$PROJECT_ID \\\n",
        "    --repository-format=docker \\\n",
        "    --location=us-central1 \\\n",
        "    --description=\"Docker repository\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BabRm8rBY43"
      },
      "outputs": [],
      "source": [
        "! gcloud builds submit \\\n",
        "  --tag us-central1-docker.pkg.dev/$PROJECT_ID/pytorch-t4/swin-train ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heZGo6uTFL5x"
      },
      "source": [
        "You need to provide repository read access to the cluster otherwise you will get permission denied errors - https://cloud.google.com/kubernetes-engine/docs/troubleshooting#permission_denied_error\n",
        "Please use the Compute Engine default service account email to update the below command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhf5-X2TFL5x"
      },
      "outputs": [],
      "source": [
        "! gcloud artifacts repositories add-iam-policy-binding pytorch-t4 \\\n",
        "    --location=us-central1 \\\n",
        "    --member=serviceAccount:<SERVICE_ACCOUNT_EMAIL> \\\n",
        "    --role=\"roles/artifactregistry.reader\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVL0E7cR6Xam"
      },
      "source": [
        "### YAML\n",
        "\n",
        "In the context of Kubernetes, a YAML (YAML Ain't Markup Language) file is a human-readable data serialization format used for configuration files. Kubernetes uses YAML files to define and configure resources such as pods, deployments, services, and more.\n",
        "\n",
        "The file typically contains a set of key-value pairs, where the keys represent the configuration options and the values represent their respective settings. The structure corresponds to the desired state of the  resources needed for a job.\n",
        "\n",
        "A YAML file for a basic Pod might look like this:\n",
        "\n",
        "```\n",
        "apiVersion: v1\n",
        "kind: Pod\n",
        "metadata:\n",
        "  name: example-pod\n",
        "spec:\n",
        "  containers:\n",
        "  - name: nginx-container\n",
        "    image: nginx:latest\n",
        "\n",
        "```\n",
        "\n",
        "We will see additional key-value pairs in the YAML necessary for our training job, including the job name, the docker image, and a graceful termination setting.\n",
        "\n",
        "YAML files are typically run with a `kubectl` command such as:\n",
        "\n",
        "`kubectl apply -f example-pod.yaml`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipXnfYmwXRy"
      },
      "source": [
        "Remember to add your project id to the .yaml file below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbKkhL8TBY7k"
      },
      "outputs": [],
      "source": [
        "%%writefile train.yaml\n",
        "\n",
        "apiVersion: batch/v1\n",
        "kind: Job\n",
        "metadata:\n",
        "  name: swin-train-job-1\n",
        "spec:\n",
        "  backoffLimit: 5\n",
        "  template:\n",
        "    metadata:\n",
        "    spec:\n",
        "      terminationGracePeriodSeconds: 60\n",
        "      containers:\n",
        "      - name: mnist-train\n",
        "        image: us-central1-docker.pkg.dev/<your_project_id>/pytorch-t4/swin-train:latest\n",
        "        resources:\n",
        "          limits:\n",
        "            nvidia.com/gpu: 1\n",
        "      restartPolicy: OnFailure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggkuZh9j7bFC"
      },
      "source": [
        "### kubectl\n",
        "\n",
        "`kubectl` is the command-line tool used for interacting with Kubernetes clusters. It serves as the primary means for administrators, developers, and operators to manage Kubernetes clusters and work with the resources within them. The name \"Kube Control.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVrcQQKhRKJ1"
      },
      "source": [
        "Let's link `kubectl` to the cluster so we can start and check the training job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1DKmT9yukW2"
      },
      "outputs": [],
      "source": [
        "!gcloud container clusters get-credentials t4-demo \\\n",
        "    --location ${REGION}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qZyvvAU3pYQ"
      },
      "source": [
        "To use `kubectl` commands on our cluster, we have to make sure it's talking to the correct one. We can use the `set-context` command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnvgHmUpuIk9"
      },
      "outputs": [],
      "source": [
        "!kubectl config set-context t4-demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6idTyy43z5s"
      },
      "source": [
        "Get the basic info about the cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai0wQo6luwnd"
      },
      "outputs": [],
      "source": [
        "!kubectl cluster-info"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lejHzJqX33hu"
      },
      "source": [
        "Run, or 'apply' the training job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQBBxsNiFfUV"
      },
      "outputs": [],
      "source": [
        "!kubectl apply -f train.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5kTaE-Y38qj"
      },
      "source": [
        "See all jobs running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAgGWxYPJtW1"
      },
      "outputs": [],
      "source": [
        "!kubectl get jobs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlW1k-_A3--7"
      },
      "source": [
        "Check the nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlJNNqydg2_m"
      },
      "outputs": [],
      "source": [
        "!kubectl get nodes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkXcY0uJ4AEF"
      },
      "source": [
        "Describe the training job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Rdt6kt5JxLV"
      },
      "outputs": [],
      "source": [
        "!kubectl describe job train-job-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxk0UolTXGk_"
      },
      "source": [
        "To inspect the pods running the job, replace the value below with the `Created pod: xxxxx` value above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBiaA6RdY15O"
      },
      "outputs": [],
      "source": [
        "!kubectl describe pod train-job-1-xxxxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KCipYw3iMqy"
      },
      "source": [
        "----------------------------------"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}