{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rastringer/ml_gke_notebooks/blob/main/introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv76sbm3U2IT"
      },
      "source": [
        "### Training a simple MNIST model on T4 GPU using GKE\n",
        "\n",
        "In this introductory notebook, we will create and run a simple model training job using a GPU on GKE.\n",
        "\n",
        "The MNIST database (Modified National Institute of Standards and Technology database) is a popular dataset for training image recognition and other machine learning models. It comprises 60,000 training images and 10,000 test images of handwritten digits, 0-9.\n",
        "\n",
        "As is common for container-based workloads, this code should be easily adapted to running elsewhere, either on a local kubernetes cluster or on another cloud.\n",
        "\n",
        "### Authenticate to GCP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23lc27zkU0SH"
      },
      "outputs": [],
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Adzxse4hVDFi"
      },
      "source": [
        "Let's make sure we have the Google Cloud CLI installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Z0KNFeSUygk"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg |  gpg --dearmor -o /usr/share/keyrings/cloud.google.gpg\n",
        "\n",
        "echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\" |  tee -a /etc/apt/sources.list.d/google-cloud-sdk.list\n",
        "\n",
        "apt-get update &&  apt-get install google-cloud-cli\n",
        "\n",
        "apt-get update && apt-get install google-cloud-cli-gke-gcloud-auth-plugin\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jekg86nZWuOk"
      },
      "source": [
        "Here's how we can install `kubectl` in the Colab environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEklYhgbWpOj"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "apt-get update && apt-get install -y apt-transport-https gnupg2\n",
        "curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | tee -a /etc/apt/sources.list.d/kubernetes.list\n",
        "apt-get update\n",
        "apt-get install -y kubectl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_354g2oMSOZd"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "gcloud config set project notebooks-370010"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WrG8o2ETxLW"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "gcloud projects describe notebooks-370010 --format=\"value(projectNumber)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eY8atHd7VOod"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "gcloud services enable iam.googleapis.com"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIuHPfl0tqhX"
      },
      "source": [
        "### Set environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Authenticate to Google Cloud\n",
        "\n",
        "To be able to run Terraform to provision your GKE cluster and supporting infrastructure we will be using your [Application Default Credentials](https://cloud.google.com/docs/authentication/provide-credentials-adc) (ADC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "gcloud auth application-default login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8tEBusj3uRH"
      },
      "source": [
        "### Create a GKE cluster with Terraform\n",
        "\n",
        "GKE Clusters can be provisioned using Terraform, allows us to adopt an Infrastrcture as Code (IaC) approach. There are many options when creating a GKE cluster, please see [here](https://cloud.google.com/sdk/gcloud/reference/container/clusters/create) for the full list.\n",
        "\n",
        "Start, by making a copy of the `terraform.tfvars.example` file and update at a minimum update the `project_id` field with the project ID you are using for your Google Cloud Project.\n",
        "\n",
        "Note that a new VPC will be provisioned to be used by the GKE cluster as part of the Terraform deployment.\n",
        "\n",
        "Once complete, run the following commands to allow Terraform to provision a new Cluster with a [node pool](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus) that has T4 GPUs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYhS2ghP0Q-d"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "terraform init\n",
        "terraform apply -auto-approve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av_qSHrm3Nay"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "apt-get update && apt-get install -y apt-transport-https gnupg2\n",
        "curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "echo \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | tee -a /etc/apt/sources.list.d/kubernetes.list\n",
        "apt-get update\n",
        "apt-get install -y kubectl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xERpM8Cd8Vn-"
      },
      "source": [
        "###Set up the convolutional neural net\n",
        "\n",
        "Here's our simple convolutional neural network in PyTorch which we will use for some investigations on the MNIST dataset of hand-drawn numbers, 0-9. It consists of:\n",
        "\n",
        "* Two convolutional layers (conv1 and conv2)\n",
        "* Two dropout layers, used for regularization\n",
        "* Two fully connected layers - these 'flatten' outputs from the pervious layers and reduce the dimensions to the final output of 10 classes in this case (numerals 0-9)\n",
        "* The 'forward' methos defines the forward pass of data through the various layers, with ReLU activation, max-pooling and dropout to prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGZ6Tiak9YAf"
      },
      "source": [
        "### Writefile\n",
        "\n",
        "We use `%%writefile`, a magic function for notebooks, to write our code to a file, `train.py` in this case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYhCYz6F-aFW"
      },
      "outputs": [],
      "source": [
        "%%writefile train.py\n",
        "\n",
        "\"\"\"\n",
        "Thanks to Meta for the PyTorch example at\n",
        "https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % LOG_INTERVAL == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "EPOCHS = 1\n",
        "BATCH_SIZE = 64\n",
        "LR = 0.001\n",
        "GAMMA = 0.7\n",
        "SEED = 1\n",
        "LOG_INTERVAL = 100\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "if use_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "train_kwargs = {'batch_size': BATCH_SIZE}\n",
        "test_kwargs = {'batch_size': BATCH_SIZE}\n",
        "if use_cuda:\n",
        "    cuda_kwargs = {'num_workers': 1,\n",
        "                    'pin_memory': True,\n",
        "                    'shuffle': True}\n",
        "    train_kwargs.update(cuda_kwargs)\n",
        "    test_kwargs.update(cuda_kwargs)\n",
        "\n",
        "transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                    transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(dataset1,**train_kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2, **test_kwargs)\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=LR)\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1, gamma=GAMMA)\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "torch.save(model, \"mnist_cnn.pt\")\n",
        "\n",
        "# Upload the trained model to Cloud storage\n",
        "from google.cloud import storage\n",
        "\n",
        "storage_client = storage.Client()\n",
        "\n",
        "BUCKET_NAME=\"genai-experiments\"\n",
        "\n",
        "destination_blob_name = \"k8s-models/mnist_cnn.pt\"\n",
        "\n",
        "bucket = storage_client.bucket(BUCKET_NAME)\n",
        "blob = bucket.blob(destination_blob_name)\n",
        "\n",
        "blob.upload_from_filename(\"mnist_cnn.pt\")\n",
        "\n",
        "print(f\"Model uploaded to: gs://{BUCKET_NAME}/{destination_blob_name}\")\n",
        "\n",
        "from google.cloud import logging\n",
        "\n",
        "client = logging.Client()\n",
        "logger = client.logger('training_job_logger')\n",
        "\n",
        "text = f\"Resource usage: CPU={cpu_usage}%, Memory={memory_usage}MB\"\n",
        "logger.log_text(text, severity='INFO')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND_IUMZiBYpc"
      },
      "outputs": [],
      "source": [
        "%%writefile Dockerfile\n",
        "\n",
        "FROM nvidia/cuda:12.2.0-runtime-ubuntu22.04\n",
        "\n",
        "RUN apt-get update && \\\n",
        "    apt-get -y --no-install-recommends install python3-dev gcc python3-pip git && \\\n",
        "    rm -rf /var/lib/apt/lists/*\n",
        "\n",
        "RUN pip3 install --no-cache-dir torch torchvision transformers peft datasets bitsandbytes protobuf scipy einops google-cloud-storage google-cloud-logging\n",
        "\n",
        "COPY train.py /train.py\n",
        "\n",
        "ENV PYTHONUNBUFFERED 1\n",
        "\n",
        "CMD python3 /train.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fycbJHyBY2L"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "gcloud artifacts repositories create pytorch-t4 \\\n",
        "    --project=$PROJECT_ID \\\n",
        "    --repository-format=docker \\\n",
        "    --location=us-central1 \\\n",
        "    --description=\"Docker repository\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2BabRm8rBY43"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "gcloud builds submit \\\n",
        "  --tag us-central1-docker.pkg.dev/$PROJECT_ID/pytorch-t4/mnist-train ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVL0E7cR6Xam"
      },
      "source": [
        "### YAML\n",
        "\n",
        "In the context of Kubernetes, a YAML (YAML Ain't Markup Language) file is a human-readable data serialization format used for configuration files. Kubernetes uses YAML files to define and configure resources such as pods, deployments, services, and more.\n",
        "\n",
        "The file typically contains a set of key-value pairs, where the keys represent the configuration options and the values represent their respective settings. The structure corresponds to the desired state of the  resources needed for a job.\n",
        "\n",
        "A YAML file for a basic Pod might look like this:\n",
        "\n",
        "```\n",
        "apiVersion: v1\n",
        "kind: Pod\n",
        "metadata:\n",
        "  name: example-pod\n",
        "spec:\n",
        "  containers:\n",
        "  - name: nginx-container\n",
        "    image: nginx:latest\n",
        "\n",
        "```\n",
        "\n",
        "We will see additional key-value pairs in the YAML necessary for our training job, including the job name, the docker image, and a graceful termination setting.\n",
        "\n",
        "YAML files are typically run with a `kubectl` command such as:\n",
        "\n",
        "`kubectl apply -f example-pod.yaml`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipXnfYmwXRy"
      },
      "source": [
        "Remember to add your project id to the .yaml file below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbKkhL8TBY7k"
      },
      "outputs": [],
      "source": [
        "%%writefile train.yaml\n",
        "\n",
        "apiVersion: batch/v1\n",
        "kind: Job\n",
        "metadata:\n",
        "  name: train-job-1\n",
        "spec:\n",
        "  backoffLimit: 2\n",
        "  template:\n",
        "    metadata:\n",
        "    spec:\n",
        "      terminationGracePeriodSeconds: 60\n",
        "      containers:\n",
        "      - name: mnist-train\n",
        "        image: us-central1-docker.pkg.dev/<YOUR-PROJECT-ID>/pytorch-t4/mnist-train:latest\n",
        "        resources:\n",
        "          limits:\n",
        "      restartPolicy: OnFailure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggkuZh9j7bFC"
      },
      "source": [
        "### kubectl\n",
        "\n",
        "`kubectl` is the command-line tool used for interacting with Kubernetes clusters. It serves as the primary means for administrators, developers, and operators to manage Kubernetes clusters and work with the resources within them. The name \"Kube Control.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVrcQQKhRKJ1"
      },
      "source": [
        "Let's link `kubectl` to the cluster so we can start and check the training job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1DKmT9yukW2"
      },
      "outputs": [],
      "source": [
        "!gcloud container clusters get-credentials t4-demo \\\n",
        "    --location us-central1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PnvgHmUpuIk9"
      },
      "outputs": [],
      "source": [
        "!kubectl config set-context t4-demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ai0wQo6luwnd"
      },
      "outputs": [],
      "source": [
        "!kubectl cluster-info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQBBxsNiFfUV"
      },
      "outputs": [],
      "source": [
        "!kubectl apply -f train.yaml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAgGWxYPJtW1"
      },
      "outputs": [],
      "source": [
        "!kubectl get jobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GlJNNqydg2_m"
      },
      "outputs": [],
      "source": [
        "!kubectl get nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Rdt6kt5JxLV"
      },
      "outputs": [],
      "source": [
        "!kubectl describe job train-job-1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxk0UolTXGk_"
      },
      "source": [
        "To inspect the pods running the job, replace the value below with the `Created pod: xxxxx` value above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBiaA6RdY15O"
      },
      "outputs": [],
      "source": [
        "!kubectl describe pod train-job-1-xxxxx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhcbWNSozDaA"
      },
      "source": [
        "Finally, check the model file is in the specified storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gvf2ez-y8Hn"
      },
      "outputs": [],
      "source": [
        "!gsutil ls gs://{BUCKET_NAME}/k8s-models/mnist_cnn.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KCipYw3iMqy"
      },
      "source": [
        "----------------------------------"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPzsx6IPASxvBlbzSFkNzrc",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
